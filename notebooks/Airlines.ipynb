{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import sys\n",
    "stdi, stdo, stde = sys.stdin, sys.stdout, sys.stderr\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdin, sys.stdout, sys.stderr = stdi, stdo, stde\n",
    "\n",
    "def geturl(url):\n",
    "    headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36'}\n",
    "    hbpage = requests.get(url, headers=headers).text\n",
    "    #print hbpage\n",
    "    return hbpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://flight.qunar.com/schedule/alphlet_order.jsp\"\n",
    "headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36'}\n",
    "hbpage = requests.get(url, headers=headers).content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "阿克苏 阿勒泰 安庆 包头 保山 北海  北京  百色 长春 长沙 长治 常德 常州 成都 赤峰  重庆 昌都 朝阳 达州 大理 大连 丹东 东营 敦煌 大同 大庆 鄂尔多斯 恩施 福州 阜阳 赣州 格尔木 广州 贵阳 桂林 哈尔滨 海口 哈密 海拉尔 邯郸 汉中 杭州 合肥 和田 黑河 呼和浩特 黄山 黄龙 济南 济宁 九江 佳木斯 嘉峪关 锦州 井冈山 景德镇 晋江 鸡西 喀什 库尔勒 昆明 克拉玛依 拉萨 兰州 丽江 连云港 临沧 临沂 柳州 泸州 洛阳 满洲里 芒市 绵阳 牡丹江 梅县 漠河 南昌 南充 南京 南宁 南通 南阳 宁波 攀枝花 齐齐哈尔 秦皇岛 青岛 衢州 三亚 汕头 上海 深圳 沈阳 石家庄 思茅 塔城 太原 台州 天津 通辽 铜仁 天水 万州 威海 潍坊 温州 文山 乌海 乌兰浩特 乌鲁木齐 无锡 武汉 武夷山 梧州 西安 西昌 西宁 西双版纳 锡林浩特 厦门 襄樊 香格里拉 徐州 兴义 烟台 延安 延吉 盐城 伊宁 宜宾 宜昌 义乌 银川 永州 榆林 运城 伊春 玉树 湛江 张家界 昭通 郑州 舟山 珠海 芷江 \n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(hbpage, 'html.parser')\n",
    "data_raw = soup.find_all(\"div\", attrs={\"class\": \"apl_order\"})\n",
    "data_raw = soup.find(\"div\", attrs={\"class\": \"js_tabSwitch\"})\n",
    "items = data_raw.find_all(\"li\")\n",
    "city_list = []\n",
    "for item in items:\n",
    "    city_name = item.get_text()\n",
    "    print city_name,\n",
    "    city_list.append(city_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_pre = \"http://flight.qunar.com/schedule/alph_order.jsp?city=\"\n",
    "p = requests.get(url_pre+city_list[0], headers=headers).content\n",
    "soup = BeautifulSoup(p, 'html.parser')\n",
    "data_raw = soup.find('ul', attrs={\"class\": \"apl_citylist fix\"})\n",
    "data = data_raw.find_all('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "阿克苏 阿勒泰 安庆 包头 保山 北海  北京  百色 长春 长沙 长治 常德 常州 成都 赤峰  重庆 昌都 朝阳 达州 大理 大连 丹东 东营 敦煌 大同 大庆 鄂尔多斯 恩施 福州 阜阳 赣州 格尔木 广州 贵阳 桂林 哈尔滨 海口 哈密 海拉尔 邯郸 汉中 杭州 合肥 和田 黑河 呼和浩特 黄山 黄龙 济南 济宁 九江 佳木斯 嘉峪关 锦州 井冈山 景德镇 晋江 鸡西 喀什 库尔勒 昆明 克拉玛依 拉萨 兰州 丽江 连云港 临沧 临沂 柳州 泸州 洛阳 满洲里 芒市 绵阳 牡丹江 梅县 漠河 南昌 南充 南京 南宁 南通 南阳 宁波 攀枝花 齐齐哈尔 秦皇岛 青岛 衢州 三亚 汕头 上海 深圳 沈阳 石家庄 思茅 塔城 太原 台州 天津 通辽 铜仁 天水 万州 威海 潍坊 温州 文山 乌海 乌兰浩特 乌鲁木齐 无锡 武汉 武夷山 梧州 西安 西昌 西宁 西双版纳 锡林浩特 厦门 襄樊 香格里拉 徐州 兴义 烟台 延安 延吉 盐城 伊宁 宜宾 宜昌 义乌 银川 永州 榆林 运城 伊春 玉树 湛江 张家界 昭通 郑州 舟山 珠海 芷江 \n"
     ]
    }
   ],
   "source": [
    "hblist = []\n",
    "for i in range(len(city_list)):\n",
    "    print city_list[i],\n",
    "    p = requests.get(url_pre+city_list[i], headers=headers).content\n",
    "    soup = BeautifulSoup(p, \"html.parser\")\n",
    "    data_raw = soup.find(\"ul\", attrs={\"class\": \"apl_citylist fix\"})\n",
    "    items = data_raw.find_all(\"li\")\n",
    "    for item in items:\n",
    "        hblist.append(item.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urlist = []\n",
    "full = \"http://flight.qunar.com/schedule/fsearch_list.jsp?departure=北京&arrival=大同\"\n",
    "pre = \"http://flight.qunar.com/schedule/fsearch_list.jsp?departure=\"\n",
    "for i in hblist:\n",
    "    a, b = i.split(\"-\")\n",
    "    urlist.append(pre+a+\"&arrival=\"+b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thefile = open('city_list.txt', 'w')\n",
    "\n",
    "\n",
    "for i in city_list:\n",
    "    thefile.write(\"%s\\n\" % url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flyin(city_citys,numm):\n",
    "    print '>>>>>进程%d开始>>>>>'%(numm+1)\n",
    "    # time.sleep(5)\n",
    "    pageinfo = []\n",
    "    for city_city in city_citys:\n",
    "        hbpage=geturl(city_city)\n",
    "        # print hbpage\n",
    "        bg=str(city_city.split('=')[-2]).replace('&arrival','')#始发\n",
    "        dg=city_city.split('=')[-1]#飞往\n",
    "        ft=(bg+dg)\n",
    "        print ft\n",
    "        if BeautifulSoup(hbpage, 'lxml').select('p[class=\"msg2\"]'):\n",
    "            print '此线路无航班'\n",
    "        else:\n",
    "            hblc = BeautifulSoup(hbpage, 'lxml').select('em')[0].get_text()\n",
    "            print hblc\n",
    "            #航班班次 第一列\n",
    "            hbtitle=BeautifulSoup(hbpage,'lxml').select('span[class=\"title\"]')\n",
    "            hbbc1=[]\n",
    "            hkgs1=[]\n",
    "            jixing1=[]\n",
    "            for ht in hbtitle:\n",
    "                hbbc=re.findall(r'\\w.?\\d*',ht.get_text())[0]#航班班次\n",
    "                hkgs=ht.get_text().split(hbbc)[0]#航空公司\n",
    "                jixing=ht.get_text().split(hbbc)[1]#机型\n",
    "                hbbc1.append(hbbc)\n",
    "                hkgs1.append(hkgs)\n",
    "                jixing1.append(jixing)\n",
    "            #起降时间 第二列\n",
    "            hbtime=BeautifulSoup(hbpage,'lxml').select('span[class=\"c2\"]')[1:]\n",
    "            qfsj1=[]\n",
    "            jlsj1=[]\n",
    "            for qj in hbtime:\n",
    "                qfsj=qj.get_text()[:5]#起飞时间\n",
    "                jlsj=qj.get_text()[5:]#降落时间\n",
    "                qfsj1.append(qfsj)\n",
    "                jlsj1.append(jlsj)\n",
    "\n",
    "            #机场 第三列\n",
    "            hbjc=BeautifulSoup(hbpage,'lxml').select('span[class=\"c3\"]')[1:]\n",
    "            qfjc1=[]\n",
    "            jljc1=[]\n",
    "            for jc in hbjc:\n",
    "                qfjc=jc.get_text().split('机场'.decode('utf-8'))[0]+'机场'.decode('utf-8')#起飞机场\n",
    "                jljc=jc.get_text().split('机场'.decode('utf-8'))[1]+'机场'.decode('utf-8')#降落机场\n",
    "                qfjc1.append(qfjc)\n",
    "                jljc1.append(jljc)\n",
    "\n",
    "            #准点率 第四列\n",
    "            zdl=BeautifulSoup(hbpage,'lxml').select('span[class=\"c4\"]')[1:]\n",
    "            zdl1=[]\n",
    "            wdsj1=[]\n",
    "            for zd in zdl:\n",
    "                zdlv=zd.get_text().split('%')[0]+'%'\n",
    "                wdsj=zd.get_text().split('%')[1]\n",
    "                zdl1.append(zdlv)\n",
    "                wdsj1.append(wdsj)\n",
    "\n",
    "\n",
    "            #班期 第五列\n",
    "            bq=BeautifulSoup(hbpage,'lxml').select('span[class=\"c5\"]')[1:]\n",
    "            # xx=[]\n",
    "            duty1 = []\n",
    "            for xq in bq:\n",
    "                week=re.split('\\\">\\d</span>',str(xq))[:-1]\n",
    "                week1=[]\n",
    "                ii=1\n",
    "                for day in week:\n",
    "                    if ii==1:\n",
    "                        dd='周一'\n",
    "                    elif ii==2:\n",
    "                        dd = '周二'\n",
    "                    elif ii==3:\n",
    "                        dd = '周三'\n",
    "                    elif ii==4:\n",
    "                        dd = '周四'\n",
    "                    elif ii==5:\n",
    "                        dd = '周五'\n",
    "                    elif ii==6:\n",
    "                        dd = '周六'\n",
    "                    elif ii==7:\n",
    "                        dd = '周日'\n",
    "                    if day[-4:]=='blue':\n",
    "                        duty='%s有班期'%dd\n",
    "                    else:\n",
    "                        duty='%s没有班期'%dd\n",
    "                    ii+=1\n",
    "                    week1.append(duty)\n",
    "                duty1.append(week1)\n",
    "\n",
    "            #班期有效期 第七列\n",
    "            bqyxq = BeautifulSoup(hbpage, 'lxml').select('span[class=\"c7\"]')[1:]\n",
    "            qs1=[]\n",
    "            js1=[]\n",
    "            for yxq in bqyxq:\n",
    "\n",
    "                yxs=yxq.get_text()[:10]#起始\n",
    "                yxe=yxq.get_text()[:10]#结束\n",
    "                qs1.append(yxs)\n",
    "                js1.append(yxe)\n",
    "\n",
    "            print hbbc1\n",
    "            for li in range(len(hbbc1)):\n",
    "                print '标记'\n",
    "                pageinfo.append(bg + ',' + dg + ',' + hblc + ',' + hbbc1[li] + ',' + hkgs1[li] + ',' + jixing1[li] + ',' + qfsj1[li] + ',' + jlsj1[li] + ',' + qfjc1[li] + ',' + jljc1[li] + ',' + zdl1[li] + ',' + wdsj1[li] + ',' + duty1[li][0] + ',' + duty1[li][1] + ',' + duty1[li][2] + ',' +duty1[li][3] + ',' + duty1[li][4] + ',' + duty1[li][5] + ',' + duty1[li][6] + ',' + qs1[li] + ',' + js1[li] + '\\n')\n",
    "                print bg + ',' + dg + ',' + hblc + ',' + hbbc1[li] + ',' + hkgs1[li] + ',' + jixing1[li] + ',' + qfsj1[li] + ',' + jlsj1[li] + ',' + qfjc1[li] + ',' + jljc1[li] + ',' + zdl1[li] + ',' + wdsj1[li] + ',' + duty1[li][0] + ',' + duty1[li][1] + ',' + duty1[li][2] + ',' +duty1[li][3] + ',' + duty1[li][4] + ',' + duty1[li][5] + ',' + duty1[li][6] + ',' + qs1[li] + ',' + js1[li] + '\\n'\n",
    "    print pageinfo\n",
    "    return pageinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def urlsplit(urls,jiange):\n",
    "    urli=[]\n",
    "    for newj in range(0,len(urls),jiange):\n",
    "        urli.append(urls[newj:newj+jiange])\n",
    "    return urli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    finalre=[]\n",
    "    files = open(r'C:\\Users\\username\\Desktop\\url.txt', 'r')\n",
    "    urls=[]\n",
    "    for url in files.read().split():\n",
    "        urls.append(url)\n",
    "    urlss=urls[:6]\n",
    "    urlll=urlsplit(urlss,2)\n",
    "    p = Pool(2)\n",
    "    result=[]\n",
    "    for i in range(len(urlll)):\n",
    "        result.append(p.apply_async(flyin,args=(urlll[i],i)))\n",
    "    p.close()\n",
    "    p.join()\n",
    "    print result\n",
    "    for result_i in range(len(result)):\n",
    "        print result[result_i]\n",
    "        fin_info_result_list = result[result_i].get().decode('utf-8')\n",
    "        finalre.extend(fin_info_result_list)\n",
    "    filers=open(r'C:\\Users\\username\\Desktop\\url55.txt','a+')\n",
    "    for fll in finalre:\n",
    "        filers.write(str(fll).encode('utf-8'))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
